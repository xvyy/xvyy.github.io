---
bg: photo025.jpg
layout: post
title: Deep HRNet for HPE
crawlertitle: Deep High-Resolution
summary: HRNet for HPE
date: 2019-09-02 22:00:00 +0800
categories: paper
tags: HPE
author: vpromise
---

*Paper Reading: Deep High-Resolution Representation Learning for Human Pose Estimation*

---

- [Deep High-Resolution Representation Learning for Human Pose Estimation](#deep-high-resolution-representation-learning-for-human-pose-estimation)
  - [介绍](#介绍)
  - [贡献](#贡献)
  - [方法](#方法)
    - [高分辨率网络](#高分辨率网络)
    - [重复多尺度融合。](#重复多尺度融合)
  - [实验](#实验)
    - [标准数据集性能](#标准数据集性能)
    - [重复多尺度融合](#重复多尺度融合-1)
  - [总结](#总结)

# Deep High-Resolution Representation Learning for Human Pose Estimation

本文由中国科学技术大学和微软亚洲研究院视觉计算组完成，提出高分辨率深度神经网络（HRNet），与上篇论文 Multi-Person Pose Estimation with Enhanced Channel-wise and Spatial Information 并列为今年 Look Into Person (LIP) 竞赛 Single-Person Human Pose Estimation 挑战赛的冠军。

## 介绍
对于视觉识别中的区域层次和像素层次问题，分类网络（如ResNet、VGGNet等）学到的表征分辨率比较低，在此基础上恢复的高分辨率表征空间区分度仍然不够强，使其在对空间精度敏感的任务上很难取得准确的预测结果。本文提出高分辨率深度神经网络（HRNet），对网络结构做了基础性的改变，由传统的串行连接高低分辨率卷积，改成并行连接高低分辨率卷积，通过全程保持高分辨率和对高低分辨率表征的多次信息交换来学到丰富的高分辨率表征，在多个数据集的人体姿态估计任务中取得了最佳的性能。

## 贡献
本文的主要贡献可以总结为以下两点：
1. 提出将高分辨率到低分辨率的子网络并联，而不是像大多数现有的串联解决方案，从而此方法能够保持高分辨率，而不是通过从低到高的过程恢复分辨率，因此预测的热图在空间上可能更精确。
2. 大多数现有的融合方案集合了低级和高级表示。而本文执行重复的多尺度融合并借助于相同深度和相似级别的低分辨率表示来增强高分辨率表示，从而高分辨率表示对于姿态估计也提供了丰富的信息。

## 方法

### 高分辨率网络
HRNet 的结构如图1所示，HRNet整个网络中始终保持高分辨率表征，逐步引入低分辨率卷积，并且将不同分辨率的卷积并行连接。同时，通过不断在多分辨率表征之间进行信息交换，来提升高分辨率和低分辨率表征的表达能力，让多分辨率表征之间更好地相互促进。HRNet 将高低分辨率并行连接是较大的创新。

![图1 HRNet 结构图](https://i.loli.net/2019/09/02/uQs8Cf23zLIRWeB.png)

<center>图1 HRNet 结构图</center>

![图2 多分辨率信息交换](https://i.loli.net/2019/09/02/kojQit8Y4wCXVxl.png)
<center>图2 多分辨率信息交换</center>

### 重复多尺度融合。
论文在并行子网中引入交换单元，这样每个子网可以重复接收来自其他并行子网的信息。多分辨率表征信息交换如图2所示，每一个分辨率的输出表征都会融合三个分辨率输入的表征，以保证信息的充分利用和交互。将高分辨率特征降到低分辨率特征时，采用stride为2的3x3卷积；低分辨率特征到高分辨率特征时，先利用1x1卷积进行通道数的匹配，再利用最近邻插值的方式来提高分辨率。相同分辨率的表征则采用恒等映射的形式。

## 实验
### 标准数据集性能
本文在 COCO Keypoint Detection、MPII Human Pose Estimation 和 Pose Tracking 三个数据集上均做了大量的实验验证 HRNet 的性能。如下表1 展示了在 COCO test-dev上与最先进方法的性能比较，可以看到小模型HRNet-W32和大模型HRNet-W48都取得了性能提升，在引入额外数据的情况下，大模型展现了更强的表达能力，有更显著的提升。在其他数据集上也展示了很好的性能，在此不做展示了。

<center>表1 Comparisons on the COCO validation set</center>

![表1 Comparisons on the COCO validation set](https://i.loli.net/2019/09/02/pMTv6KVlfeUzHZ9.png)

### 重复多尺度融合
本文分析了重复多尺度融合的效果，研究了网络的三种变体，如表2所示。验证集的结果表明，多分辨率表征信息交换可以将不同分辨率的表征信息进行充分的交换利用，对表征增强的作用十分明显，可以到达2.6% AP的提升。

<center>表2 多尺度融合研究</center>

![表2 多尺度融合研究](https://i.loli.net/2019/09/02/ktnzWVK7Igr6YDA.png)

###表征分辨率
本文研究了表征分辨率如何影响姿态估计性能：从高到低检查从每个分辨率的特征映射估计的热图质量，从图3 可以清楚地看到，网络输出表征的分辨率降低会使得模型的性能有巨大的损失。这体现了表征分辨率对于空间精度的重要性。

![图3 表征分辨率研究](https://i.loli.net/2019/09/02/CNku9sL4vUjKaZ6.png)
<center>图3 表征分辨率研究</center>

## 总结
视觉识别主要包括三大类问题：图像层次（图像分类），区域层次（目标检测）和像素层次（图像分割、人体姿态估计等）。用于图像分类的卷积神经网络成为解决视觉识别问题的标准结构。这类网络的特点是学到的表征在空间分辨率上逐渐变小。但分类网络并不适合区域层次和像素层次的问题，因为学到的表征本质上具有低分辨率的特点，在分辨率上的巨大损失使得其在对空间精度敏感的任务上很难取得准确的预测结果。

为了弥补空间精度的损失，在分类卷积神经网络结构的基础上，通过引入上采样操作和/或组合空洞卷积减少降采样次数来提升表征的分辨率，典型的结构包括Hourglass、U-Net等。在这类网络结构中，最终的高分辨表征主要来源于两个部分：第一是原本的高分辨率表征，但是由于只经过了少量的卷积操作，其本身只能提供低层次的语义表达；第二是低分辨率表征通过上采样得到的高分辨率表征，其本身虽然拥有很好的语义表达能力，但是上采样本身并不能完整地弥补空间分辨率的损失。所以，最终输出的高分辨率表征所具有的空间敏感度并不高，很大程度上受限于语义表达力强的表征所对应的分辨率。

本论文改变了现有的基于分类网络的人体姿态估计的网络结构，提出了高分辨率深度神经网络（HRNet）。整个过程保持高分辨率，不需要从低分辨率恢复高分辨率，以及多次对高低分辨率表征进行信息补足，成功学到足够丰富的高分辨率表征。

HRNet 不仅对人体姿态估计任务有着很好的性能表现，目前 HRNet 在图像分割、目标检测、图像分类等等任务上都展现了很好的性能，是大家可以去尝试的一个方法。具体可以参考 [GitHub HRNet](https://github.com/HRNet) 页面 。最近，本文作者又放出新论文 Bottom-up Higher-Resolution Networks for Multi-Person Pose Estimation，是 HRNet 的升级版。

新论文中作者采用自下而上的方法进行多人姿态估计。典型的自下而上流程包括热图预测和关键点分组两步。作者主要关注提高热图预测精度。提出了 HigherHRNet。通过对HRNet输出的高分辨率特征图进行解卷积来生成高分辨率特征图，这对于中小尺寸人体姿态估计来说在空间上更加精确。然后构建高质量的多层次特征并进行多尺度姿态预测。HigherHRNet 性能超越了 COCO 数据集上所有现有的自下而上的方法。